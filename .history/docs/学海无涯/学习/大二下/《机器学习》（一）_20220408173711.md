# 《机器学习》（一）

-  **绪论**

-  **模型评估与选择**

    -  生成训练集 S 和测试集 T

        -  留出法

            直接将数据集D划分为两个互斥的集合

            -  尽量保持数据的分布一致

                -  分层采样

            -  单次使用留出法的估计往往不可靠，一般要采用若干次随机划分，重复试验取平均值

        -  交叉验证法

            -  p次k折验证

            -  k=样本数量时——留一法

                被认为是比较准确的

        -  自助法

            -  自助采样——约有36.8%的样本未出现在采样数据（训练集）中

                包外估计(out-of-bag estimate)

    -  性能度量

        -  错误率与精度

        -  查准率（precision）与查全率（recall）

            查准率亦称准确率，查全率亦称召回率

            -  

                ![](media/f33fb4175b6f811c4f2ed72fee6e391b.png)

            -  查全率与查准率是一对矛盾的度量，通常只有在一些简单任务中，才会两个都很高

        -  P-R曲线（P-R图）

            -  

                ![](media/870c3c53b067da1e63ee77ff0e2808a-png)

            -  根据学习器的预测结果对样例进行排序，排在前面的认为最有可能是正例

            -  查准率为纵轴，查全率为横轴

            -  平衡点（BEP）

                查全率=查准率时的取值，综合考虑查准率与查全率的性能度量

            -  若一个学习器的P-R曲线被另一个学习器的曲线完全包住，则可断言后者性能更优

        -  F1度量：$\frac{1}{F1}=\frac{1}{2}*(\frac{1}{P}+\frac{1}{R})$

            F1是基于查准率与查全率的调和平均

            -  一般形式$F_{\beta}$: $F_\beta=\frac{(1+\beta^2)*P*R}{(\beta^2*P)+R}$，$\beta>0$度量了查全率对查准率的相对重要性，$\beta>1$时查全率有更大影响，$\beta<1$时查准率有更大影响

        -  在多个混淆矩阵中综合考虑查准率与查全率

            -  宏F1（macro-F1）:分别计算再取平均

            -  微F1（micro-F1）：对应元素分别平均再计算

        -  ROC与AUC

            -  通常使用有限个测试样例绘制ROC曲线，无法产生光滑曲线

                ![](media/5158bdaab7a5d8c39649eed7802052f0.png)

            -  ROC（受试者工作特征）

                -  真正例率（TPR）为纵轴，假正例率（FPR）为横轴

                -  TPR：敏感性Sensitivity（召回率）

                    关注的是正例

                -  FPR：（1-FPR）又称特异度Specificity

                    特异度关注的是负例

            -  AUC（Area Under ROC Curve）

                -  介于0.5—1之间

        -  参考：@https://[zhuanlan.zhihu.com/p/46714763](http://zhuanlan.zhihu.com/p/46714763)

        -  代价敏感错误率与代价曲线

            -  **非均等代价——代价敏感**

                之前的一些性能度量都隐含了假设了均等代价

            -  代价敏感性能度量（类似于加权）

            -  代价曲线

                ![](media/6f65a9320af49421716430496fea8f1-png)

                ![](media/a57967ae69658ccef0571bbd81af717e.png)

        -  比较检验

            -  假设检验

                -  二项检验（二项分布）

                -  t检验（t-test）（t分布）

                -  交叉检验t检验（成对t检验）

                    基本思想：若两个学习器的性能相同，则它们使用相同训练集\测试集得到的测试错误率应相同

                -  McNemar检验（$\chi^2$分布）

        - AIC与BIC

    -  偏差与方差

-  **线性模型**

    -  基本形式

        -  属性的线性组合：f(x)=w\^Tx+b

        -  具有很好的可解释性

            w直观表达了各属性在预测中的重要性

    -  线性回归

        -  离散属性

            -  属性间存在“序”关系，可通过连续化将其转化为连续值

            -  属性间不存在“序”关系，假设有k个属性值，**通常转化为k维向量（类似于one-hot）**

        -  均方误差最小化（最小二乘法）

        -  多元线性回归

            ![](media/87130b7c9b465d48ae7293b22ae84b1-png)

            ![](media/56f29b05f004cf364f9383c899dfdcb-png)

            ![](media/2826b0ba78c31c9b610150eca809fe0-png)

        -  广义线性模型（GLM）

            -  GLM

                ![](media/23bd6530c897a3d874e24c86e5be629-png)

            -  若用于分类任务：只需找到一个**单调可微函数**将分类任务的真实标记y与线性回归模型产生的预测值联系起来

    -  对数几率回归

        -  几率（odds）与对数几率（log odds）

        -  **用线性回归的预测结果取逼近真实标记的对数几率**

            ![](media/cb6b8f791f3a9807369b7d64f513eabe.png)

    -  线性判别分析（LDA）

    -  多分类学习

    -  **类别不平衡问题**

        分类任务中不同类别的训练样本例数目差别很大的情况

        -  线性分类器工作

            ![](media/ff914bb6372a385f2b7490ec3d5f402e.png)

        -  策略：再缩放（rescaling）

            ![](media/338d1a6b627d16501c410f8ab36aa91c.png)

            根据类别样本数的分布对分类的阈值进行缩放——阈值移动

            -  过采样oversampling

                增加数量较少类别的样本数，使样本数量相当再进行学习

            -  欠采样undersampling

                减少数量较多的类别的样本数，使样本数量相当再进行学习

            -  参考：imblearn中的算法

-  **决策树**

一个在之前决策的基础上不断做决策的过程

-  基本流程

    ![](media/2d7f11b8430f44c8d80ee6d01038e7eb.png)

    基本流程遵循“分而治之”策略

    -  划分选择

        分支节点所包含的样本尽可能属于同一类别，节点的纯度越来越高

        -  信息增益

            -  信息熵（information entropy）

                $Ent(D)=-\sum_{k=1}^{|y|}p_klog_{2}p_k$
                其中，$p_k$为当前集合D中的第k 类样本所占的比例
                $Ent(D)$的值越小，则D的纯度越高

            -  信息增益（information gain）

                $Gain(D,a)=Ent(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)$
                信息增益越大，使用属性a来进行划分所获得的“纯度提升”越大
                实际上，信息增益准则对可取值数目较多的属性有所偏好

            -  增益率（gain ratio）

                $Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$ 其中
                $IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$称为属性a的“固有值”
                属性a的可能取值数目越多（V越大），则$IV(a)$的值越大
                增益率准则对取值数目较少的属性有所偏好

            -  基尼指数（Gini index）

                $Gini(D)=\sum_{k=1}^{|y|}\sum_{k^{'}\neq{k}}p_kp_{k^{'}}=1-\sum_{k=1}^{|y|}p_k^2$,
                $Gini(D)$反映了从数据集D中随机抽取两个样本，其标记不一致的概率
                $Gini(D)$越小，数据集D的纯度越高

    -  剪枝处理

        剪枝是决策树学习算法应对过拟合的主要手段

        -  预剪枝（prepruning）

            在决策树生成过程中，对每个结点在划分前进行估计，若当前划分的节点不能带来泛化性能提升，则停止划分当前节点

            -  预剪枝基于“贪心”本质，给预剪枝决策树带来了欠拟合的风险

                有些分支的当前划分虽然不能带来性能提升，但在之后的划分中可能会带来显著提升

        -  后剪枝（postpruning）

            先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若该节点对应的子树替换为叶节点能带来性能提升，则将该子树替换为叶节点

            -  保留的分支较预剪枝的多，训练时间开销要大得多

    -  连续与缺失值（C-5）

        决策树基于离散属性生成

        -  连续值处理

            -  数据离散化

                -  分箱

                -  卡方分裂

        -  缺失值处理

            -  利用完整的数据，赋予样本不同的权值

    -  多变量决策树

        ![](media/973f974e02f9743a3b4b84f229b2abeb.png)

        非叶节点不再是仅对某个属性，而是对属性的线性组合进行测试
        换言之，每个非叶节点都是一个线性分类器

        -  单变量决策树分类边界

            ![](media/9a1c60563e0b7f8bc24b1ef55533b2b-png)

        -  多变量决策树分类边界

            ![](media/413f12eabc042e839a41edefc723d48-png)

            -  每个非叶节点不再是对某个属性，而是对属性的线性组合进行测试

                每个非叶节点是一个形如$\sum_{i=1}^{d}w_ia_i=t$的线性分类器，其中$w_i$为属性$a_i$的权重

-  **神经网络**

机器学习中的神经网络通常指的是“神经网络学习”

-  神经元模型

    -  M-P神经元模型

        ![](media/662b31574792a47858350ef409a9e40c.png)

        只有达到阈值才会产生”兴奋“向后传播信号
        注意**阈值**，阈值也需学习更新，不过一般将其并入权重之中一起学习并更新

        -  激活函数

            ![](media/b0ec57a7a7dd36566f9863656770650-png)

            与阈值比较后决定输出

    -  感知机与多层网络

        参看《统计学习方法》感知机一章

    -  **误差逆传播算法（BP算法）**

        BP算法地目标是要最小化训练集D上的累积误差：$E=\frac{1}{m}\sum_{k=1}^mE_k$
        ​

        -  标准BP算法

            ![](media/62cd33de802f92d2cf2a7330a372a0e-png)

            ![](media/5a0eb76bd5aac59ed28d6ef70ea9f70-png)

            ![](media/a4e8dc3c58450f2a1c4cee6a20a4bfa-png)

            ![](media/6cd1121b293695bbf2b1c61cd93c703-png)

            每次只针对单个样例，参数更新的非常频繁，而且对不同的样例进行更新的效果可能会出现“抵消”现象

            -  **误差**逆向传播，用于计算更新的量

                根据误差计算梯度，然后进行更新

        -  累积BP算法

            直接针对累计误差进行最小化，在读取整个训练集D一遍后才对参数进行更新，参数的更新频率较低

    -  全局最小与局部最小

        ![](media/a236adff96e4e8a7ea6417356187916-png)

        类似于极值点与最值点的关系

    -  常见神经网络

        -  RBF网络（RBF径向基函数）

            ![](media/c0f9162eae8da5c2d0cbd14369b6f11-png)

            单隐层前馈神经网络

        -  ART网络（ART自适应谐振理论）

            -  竞争型学习

                ![](media/740277e45c83dc2ea8e4cd40447d6c3-png)

            -  

                ![](media/968beeef5f370362670328879e97951-png)

                ![](media/a9c1dd5de4fd2a391fff74aa6e03dd0-png)

        -  SOM网络（SOM自组织映射）

        -  级联相关网络

        -  Elman网络

            ![](media/251ae6ead0f8a4c26c12aa76f218987-png)

            递归神经网络

        -  Boltzmann机

            ![](media/4747ee82246c696eaeb7bf95851dafa-png)

            ![](media/a4e78a978e098e7d70c743116e1a808d.png)

            基于能量的模型

            -  无监督学习

            -  隐层与显层均有偏置项

            -  Boltzmann分布

            -  概率图模型

            -  生成模型——逼近原始数据的分布

            -  DBN深度信念网络（玻尔兹曼机的堆叠）

            -  参考：@https
                [限制玻尔兹曼机（RBMs）理论详解_Alex.W.的机器学习之路-CSDN博客](https://blog.csdn.net/wangjian1204/article/details/50208841?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault--control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault--control)

-  **支持向量机**

    -  间隔与支持向量

        -  支持向量（support vector）

            距离超平面最近的几个训练样本

        -  间隔（margin）

            两个异类支持向量到超平面的距离之和

        -  

            ![](media/3e9b3a36db8c348d0970b7990a259b5-png)

        -  SVM的基本型

            ![](media/294c62b2539a762ab094f889ba35de8c.png)

            ![](media/0771be66a36cb013c6f78a15d01e5de-png)

            找到“具有最大间隔”的划分超平面

    -  **对偶问题**

        凸优化问题（强对偶性成立）的条件下，可通过构造拉格朗日函数，求偏导，并令导数为零
        可得到对偶变量与原变量的关系，再考虑约束条件，可得到对偶问题

        -  得到对偶问题

            ![](media/410148d95e12a613df1c9b0c266492d-png)

            ![](media/3a54fcd2c724d1985d9a1bc495419a8a.png)

        -  求解对偶问题

            -  对偶问题为二次规划问题，可通过通用的二次规划算法求解（开销大）

            -  SMO算法

                基本思路：先固定$\alpha_i$之外的所有参数，然后求$\alpha_i$上的极值
                由于存在约束$\sum_{i=1}^{m}\alpha_iy_i=0$，若固定$\alpha_i$之外的其他变量，则$\alpha_i$可由其他变量导出

                -  选取一对需更新的变量$\alpha_i$和$\alpha_j$

                -  固定$\alpha_i$,$\alpha_j$之外的参数，求解获得更新后$\alpha_i$,$\alpha_j$

                -  

                    ![](media/ea165802864cd8e11947b23546f42ea-png)

    -  **核函数**

        若原始样本空间中训练样本不是线性可分的，
        ​可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个空间内线性可分

        -  核矩阵（半正定）

            ![](media/a9248eec1824a3517e719f8643fd1fbd.png)

        -  常用核函数

            ![](media/94dd63eaa3ca4c02bbfd19dfe23f47c-png)

        -  通过函数组合得到核函数

            ![](media/3ec11fe702f6cbb882c8472d8f4d6a7-png)

            ![](media/d11783fa2e77c2fed40aa3e4e7c98a8-png)

    -  软间隔与正则化

        -  软间隔

            软间隔：允许支持向量机在一些样本上出错
            硬间隔：要求所有的样本严格满足约束

            -  示意图

                ![](media/1bf71af7dcfbdd224798692d7916c71-png)

            -  优化目标

                ![](media/ea36b0870c124d7118d0de7c3a18e82d.png)

                ![](media/43a9960308d2d1b2fcb21adaf112f0e-png)

                ![](media/b51a925b707390b8fce3b8347ec2e2c-png)

                在最大化间隔的同时，不满足约束的样本应该尽可能地少

                -  替代损失

                    l_{0/1}非凸，非连续，数学性质不好，使得优化目标不易求解

                    -  三种常用的替代损失函数

                        ![](media/2ff04b14713cacfdc4d78e46ac5923cc.png)

                        ![](media/1c8c2790fea42ab946970744d512477f.png)

                    -  不同的替代损失函数

    -  支持向量回归（SVR）

        假设能容忍f(x)与y之间最多有 $\epsilon$ 的偏差

        -  定义

            ![](media/b9fcb6988d9e35c016cb03b4cfda213-png)

            ![](media/6cb96ceb59c0805c5e10a69b60e8c16-png)

        -  求解——对偶问题

    -  **核方法**

        -  **表示定理**

            ![](media/4c87444ff590b960f88aa39ee802a79-png)

        -  **核化**

            引入核函数，将线性学习器拓展为非线性学习器

            -  核线性判别分析（KLDA）

                未完

    -  参见支持向量机

-  **贝叶斯分类器**

    -  贝叶斯决策论

        概率框架下实施决策的基本方法，考虑如何基于**相关概率**和**误判损失**来选择最优标记

        -  

            ![](media/3a180907c884d4cf0a3e0eed4ff7626-png)

        -  贝叶斯判定准则

            ![](media/d445f62bb546b327cb6d16d78dd5c16e.png)

            为最小化总体风险，只需在每个样本上选择那个能使条件风险**R(c\|x)**最小的类别标记

        -  贝叶斯最优分类器

            ![](media/41a18d922eb5bc37e1c5b8146d2d9f9-png)

            选择使后验概率P(c\|x)最大的类别标记

    -  估计后验概率P(c\|x)

        -  直接建模P(c\|x)——判别模型

        -  先对联合概率分布建模，再根据**贝叶斯定理**获得P(c\|x)——生成模型

            -  估计类先验概率P(c)

                通过频率估计概率

            -  估计类条件概率（似然P(x\|c)）

                由于涉及**所有关于x的属性的联合分布**，许多样本取值在训练集中可能根本没有出现，不能简单地通过频率来估计

    -  极大似然估计

        假设P(x\|c)具有确定的形式且被参数向量$\theta_c$唯一确定，参数虽然未知，却是客观存在的固定值
        **假设样本独立同分布**

        -  似然函数

            ![](media/f50c58907e9b369e63e7164ae01e539-png)

        -  对数似然函数

            ![](media/8a146c4fb0975bdb83bf2bbe5c19ce2-png)

        -  **这种参数化的方法虽然能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布**

    -  朴素贝叶斯分类器

        -  **属性条件独立性假设**

            假设每个属性独立地对分类结果发生影响，参见《统计学习方法》朴素贝叶斯分类器

    -  **半朴素贝叶斯分类器**

        对属性条件独立性假设进行一定程度上的放松

        -  基本想法

            ![](media/d51a47eea458d7907a23d40b4a2ba27c.png)

            适当考虑一部分属性间相互依赖信息，从而既不需进行联合分布概率计算，又不至于彻底忽略了比较强地属性依赖关系

        -  **独依赖估计（ODE）**

            假设每一个属性在类别之外最多仅依赖一个其他属性 ​

            -  确定父属性

                -  SPODE

                    假设所有属性都依赖于同一个属性”超父“，通过交叉验证来确定超父属性

                -  AODE

                    尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑地SPODE集成起来作为最终结果

    -  **贝叶斯网**

        ”信念网“，借助**有向无环图（DAG）**来刻画属性之间的依赖关系
        用**条件概率表（CPT）**来描述属性之间的联合概率分布

        -  结构

            ![](media/e6db760eff5c7bb1fc0d63b476ab8cad.png)

            -  属性的联合概率分布

                ![](media/e2bf5e607701cc1328fb6bcf0a5a253d.png)

            -  典型依赖关系

                ![](media/85fb65c723fdef154ffd42645e65ffb-png)

            -  条件独立性

                一个变量取值的确定与否，能对另两个变量间的独立性发生影响

                -  有向分离

                    ![](media/dd411bd1341a9d316573c2f53177638e.png)

                    -  道德图（moral graph）

                        ![](media/df97bd9cd6f934d226813d22ea33767b.png)

                        ![](media/ab7f9c9ecebc4d0403b82c373840b16a.png)

        -  学习

            -  已知网络结构

                只需对训练样本”计数“，估计出每个节点的条件概率表即可

            -  未知网络结构

                -  评分搜索

                    根据训练集来找到最”恰当“的贝叶斯网

                    -  评分函数

                        ![](media/cdb214abe78fe78675a3bf8794e56c0d.png)

                        ![](media/cad09db2e229c4d9e4389df87e3b1ede.png)

                        ![](media/6b6e5cc0fa653ac21cf09deb0b33ffd-png)

                        通常基于信息论准则

                        -  AIC评分函数

                        -  BIC评分函数

        -  推断

            通过一些属性变量的观测值来推测其他属性变量的取值

            -  近似推断

                -  吉布斯采样

                    ![](media/72d59379cd369728260dc7dde9c3f42e.png)

                    ？？？

    -  **EM算法**

        存在**隐变量**的情况下，对模型参数进行估计

        -  基本想法

            ![](media/fd415f496aba3d82b8a627fd06371f2-png)

            推断隐变量Z的期望

        -  进一步

            ![](media/6653a1a03021eeef9d75455b28cca1ad.png)

            计算隐变量的概率分布

    -  小结

        ![](media/4a6caced6886527cebb9a70792e7a71-png)

-  **集成学习**

通过构建并结合多个（弱）学习器来完成学习任务

-  集成个体应”好而不同“

    ![](media/9c6b7aa44f9751404b4380c26856e0cb.png)

    个体学习器需要有一定的准确性，而且要有多样性

    -  Boosting

        ![](media/e307f3334746402d4afe31e967603c5-png)

        个体学习器之间存在**强依赖关系**，必须**串行生成**的序列化方法
        可将弱学习器提升为强学习器的算法

        -  AdaBoost

            基于加性模型迭代式优化指数损失函数的角度推导

            -  加性模型

                $H(x)=\sum_{t=1}^T\alpha_th_t(x)$

            -  指数损失函数（用来替代0/1损失函数）

                ![](media/e0b9dfcfef6cc3e5d3580e1f7ba6d70-png)

                $l_{exp}(H\|D)=\mathbb{E}_{x\sim{D}}[e^{-f(x)H(x)}]$

            -  算法

                ![](media/2d4cc53887df733fcb4e985822268d1-png)

                -  基于初始分布训练出一个基分类器

                -  最小化经$\alpha_t$加权后的损失函数，获得权重$\alpha_t$更新（权重$\alpha_t$更新公式）

                -  调整样本分布（样本分布更新公式）

                    （使得先前分类器做错的样本后续受到更多关注？）

                -  基于调整后的样本分布训练下一个分类器

                -  每个分类器h_t都在分布D_t下最小化分类误差，且分类误差应小于0.5（类似于残差逼近的思想）

        -  从偏差-方差分解角度看，Boosting主要关注**降低偏差**，因此才能从泛化性能相当弱的学习器构造出很强的集成

    -  Bagging与随机森林

        -  Bagging

            并行式集成学习，直接基于bootstrap sampling（6-2%）

            -  算法

                ![](media/fb8df6cd2e541ce4237b06bbe2157ae-png)

            -  使用bootstrap
                sampling采样出T个采样集，基于每个采样集训练出一个基学习器

            -  预测输出结合

                -  分类任务采用简单投票法

                -  回归任务采用简单平均法

            -  优点

                ![](media/f19bfd52920943d627b30079d5f2409d.png)

            -  从偏差-方差角度看，Bagging主要关注**降低方差**，因此在**易受样本扰动**影响的学习器上效果更显著

        -  随机森林（Random Forest）

            Bagging的扩展变体，以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择

            -  对基决策树的每一个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分

                k控制了随机性的引入程度，推荐值k=log_2d，d为属性数量

            -  性能

                -  Bagging中的学习器的多样性来自于样本扰动，而RF基学习器的多样性不仅来自样本扰动，还有属性扰动，这使得泛化性能进一步提升

    -  结合策略

        -  学习器结合

            ![](media/e4b64abde26bd247ee0f7c38d3f7451-png)

        -  平均法

            -  简单平均法

            -  加权平均法

        -  投票法

            -  绝对多数投票法

            -  相对多数投票法

            -  加权投票法

            -  硬投票（学习器输出为类标记）

            -  软投票（学习器输出为类概率）

        -  学习法

            通过另一个学习器结合

            -  Stacking

                先从初始数据集训练出初始学习器，然后“生成”一个新的数据集用于训练次级学习器

                -  算法流程

                    ![](media/2003bb30cb24ba4bc7f3cb79028e92bc.png)

            -  贝叶斯模型平均（BMA）

    -  多样性

        -  误差—分歧分解

            -  分歧

                ![](media/7b7315576bddb0d061bc8d9f08ace0d-png)

                在一定程度上反映了个体学习器的多样性

        -  多样性度量

            -  不合度量

            -  相关系数

            -  Q-统计量

            -  $\kappa$-统计量

        -  多样性增强

            -  数据样本扰动

                通常基于采样法，从初始数据集中产生不同的数据子集，再利用不同的数据子集训练出不同的个体学习器

            -  输入属性扰动

                从不同子空间（属性子集）中训练出不同的个体学习器

            -  输出表示扰动

                对输出表示进行操纵以增强多样性

            -  算法参数扰动

-  **聚类**

既可作为一个单独的过程，用于寻找数据内在的分布结构，也可作为分类等其他任务的前驱过程

-  性能度量（有效性指标）

    -  外部指标

        ![](media/eeda021f1ffc7394c11208381c85b940.png)

        ![](media/b36ac8d788040fc845318485cc99f38f.png)

        与某个参考模型比较

        -  内部指标

            ![](media/499d083e52aaacba850479ac85f6a4fa.png)

            直接考察聚类结果不利用任何参考模型

    -  距离计算

        -  距离度量distance measure

            -  非负性：$dist(x_i,x_j)\le0$

            -  同一性：$dist(x_i,x_j)=0，当且仅当x_i=x_j$

            -  对称性：$dist(x_i,x_j)=dist(x_j,x_i)$

            -  直递性（三角不等式）：$dist(x_i,x_j){\le}dist(x_i,x_k)+dist(x_k,x_j)$

        -  有序属性

            -  闵可夫斯基距离

                ![](media/81ff8c37f7d188681a04a2c13024456-png)

        -  无序属性

            -  VDM（Values Difference Metrics）

                ![](media/c07d1a91b5dd3e45f8cefb46c31baa5-png)

        -  混合属性

            ![](media/501043322aa42c6d305ccbe96693f1b-png)

            将闵可夫斯基距离和VDM结合即可处理混合属性

        -  加权距离

            ![](media/d0e84187756e314c3009367ab584a5d-png)

        -  非距离度量non-metric distance

            不满足直递性

    -  原型聚类

        **“基于原型的聚类”（prototype-based clustering）
        通常情况下，算法先对原型进行初始化，然后对原型进行迭代更新求解**

        -  k均值算法（k-means）

            用原型向量（**均值向量**）来刻画聚类结构

            -  采用贪心策略，通过迭代优化来近似求解

                ![](media/d588cf89395fba09c4c324c66121213-png)

            -  算法

                ![](media/e37ddd0aaabb829bcb222ed87f29098e.png)

            -  参考 @https <https://zhuanlan.zhihu.com/p/78798251>

        -  学习向量量化（LVQ）

            用原型向量（**学习向量**）来刻画聚类结构

            -  LVQ假设数据样本带有类别标记，学习过程利用这些监督信息来辅助聚类

                ![](media/543135090f9cf036ae5c3068be409df-png)

            -  算法

                ![](media/e7cb27e129567d7fdbad20d2cd71dc9d.png)

            -  更新原型向量

                ![](media/1bd5bf1a962c25893974d1eccd75612-png)

            -  样本空间划分（Voronoi剖分）

                每个原型向量$p_i$定义了一个区域R_i,该区域中的每个样本与$p_i$的距离不大于它与其他原型向量$p_{i^{'}}$

        -  高斯混合聚类

            采用**概率模型**来表达聚类模型

            -  高斯混合分布

                ![](media/08ed998959b1ef8c8f9ef153f92a68cd.png)

            -  模型

                ![](media/1dc1406444a8dcf505b3a54c3fc7065-png)

                ![](media/0a35a22b20c668a0ff9921a039b8a49-png)

            -  模型参数求解

                -  极大似然估计

                    ![](media/6d4e486521ac0fc996fed19032a3e3a-png)

                    常利用EM算法迭代优化求解

            -  算法

                ![](media/b1173cde424cded4e42d2624fa1505b-png)

    -  密度聚类

        **“基于密度的聚类”（density-based clustering）
        此类算法假设聚类结构能通过样本分布的紧密程度确定**

        -  

            ![](media/55e04bac556b30f903a514938ba49ad-png)

        -  DBSCAN

            基于一组“邻域”参数（\\epsilon,minpoints）来刻画样本分布的紧密程度

            -  主要概念

                ![](media/9d4fc228bfd9fb1024f8af28696166ee.png)

                ![](media/60ce09eb1bb28bf30bc0ee0a588206b-png)

            -  簇的定义

                ![](media/2dbd573b7676b795d06e261e0a4e7ee-png)

            -  算法

                ![](media/c485a8250934903333da6aed4c626fca.png)

    -  层次聚类

        再不同层次对数据集进行划分，从而形成树形的聚类结构

        -  AGNES

            采用**自底向上**聚合策略的层次聚类算法

            -  先将数据集中每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个簇进行合并，该过程不断重复，知道达到预设的簇的个数

            -  关键在于簇之间的距离计算

                ![](media/a77842e6f10613b9cc5bb1dfce57b67f.png)

                -  d_{min}——单链接（single-linkage）

                -  d_{max}——全链接（complete-linkage）

                -  d_{avg}——均链接（average-linkage）

            -  算法

                ![](media/d1c4ad66869ad5187a77a9eda331843f.png)
